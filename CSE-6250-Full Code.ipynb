{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Input, MaxPool2D\n",
    "from keras.layers import Conv2D, GlobalAveragePooling1D, MaxPooling2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "## our imports\n",
    "import pandas as pd\n",
    "import re\n",
    "from itertools import chain, repeat, islice\n",
    "import requests\n",
    "\n",
    "## utility functions\n",
    "def pad_infinite(iterable, padding=None):\n",
    "   return chain(iterable, repeat(padding))\n",
    "\n",
    "def pad(iterable, size, padding=None):\n",
    "   return islice(pad_infinite(iterable, padding), size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:13:39.246799500Z",
     "start_time": "2024-03-25T03:13:39.172350700Z"
    }
   },
   "id": "ab31492270c2c5cd",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Using the same variable declarations and configuration options as the original study\n",
    "ip_txt_file = './data/500_Reddit_users_posts_labels.csv'  # CSV file: \"User\", \"Post\", \"Label\"\n",
    "ip_feat_file = './data/External_Features.csv'             # CSV file: \"User\", \"Features\"\n",
    "limit_rows = 125   ## used to build a partial dataset\n",
    "\n",
    "w2v_file = {'file': './out/numberbatch-en-19.08.txt.gz', \n",
    "            'is_binary': False, \n",
    "            'limit': None, \n",
    "            'cache': './out/vectors.kv', \n",
    "            'source':'https://conceptnet.s3.amazonaws.com/downloads/2019/numberbatch/numberbatch-en-19.08.txt.gz' }\n",
    "\n",
    "op_file = './out/Result_5-Label_Classification.tsv'\n",
    "severity_classes = {'Supportive': 0, 'Indicator': 1, 'Ideation': 2, 'Behavior': 3, 'Attempt': 4}\n",
    "\n",
    "sys_params = {'emb_dim': 300,\n",
    "              'max_sent_len': 1500,\n",
    "              'str_padd': '@PADD',\n",
    "              'cross_val': 5}\n",
    "\n",
    "cnn_params = {'no_filters': 100,\n",
    "              'kernels': [3, 4, 5],\n",
    "              'channel': 1,\n",
    "              'c_stride': (1, sys_params['emb_dim']),\n",
    "              'pad': 'same',\n",
    "              'ip_shape': (sys_params['max_sent_len'], sys_params['emb_dim'], 1),\n",
    "              'c_activ': 'relu',\n",
    "              'drop_rate': 0.3,\n",
    "              'dense_1_unit': 128,\n",
    "              'dense_2_unit': 128,\n",
    "              'dense_activ': 'relu',\n",
    "              'op_unit': 5,             # 5-Label classification\n",
    "              'op_activ': 'softmax',\n",
    "              'l_rate': 0.001,\n",
    "              'loss': 'categorical_crossentropy',\n",
    "              'batch': 4,\n",
    "              'epoch': 50,\n",
    "              'verbose': 1}\n",
    "\n",
    "intermediate_layer = 'flat_drop'    # for extracting features from CNN"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:13:39.270806700Z",
     "start_time": "2024-03-25T03:13:39.233398400Z"
    }
   },
   "id": "96ae94470d964fc4",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_keras_cnn_model():\n",
    "\n",
    "    l_ip = Input(shape=(sys_params['max_sent_len'], sys_params['emb_dim'], 1), dtype='float32')\n",
    "    lst_convfeat = []\n",
    "    for filter in cnn_params['kernels']:\n",
    "        l_conv = Conv2D(filters=cnn_params['no_filters'], kernel_size=(filter, sys_params['emb_dim']), strides=cnn_params['c_stride'],\n",
    "                        padding=cnn_params['pad'], data_format='channels_last', input_shape=cnn_params['ip_shape'],\n",
    "                        activation=cnn_params['c_activ'])(l_ip)\n",
    "        l_pool = MaxPool2D(pool_size=(sys_params['max_sent_len'], 1))(l_conv)\n",
    "        lst_convfeat.append(l_pool)\n",
    "        \n",
    "    l_concat = Concatenate(axis=1)(lst_convfeat)\n",
    "    l_flat = Flatten()(l_concat)\n",
    "    l_drop = Dropout(rate=cnn_params['drop_rate'], name='flat_drop')(l_flat)\n",
    "    l_op = Dense(units=cnn_params['op_unit'], activation=cnn_params['op_activ'], name='cnn_op')(l_drop)\n",
    "    \n",
    "    final_model = Model(l_ip, l_op)\n",
    "    final_model.compile(optimizer=Adam(learning_rate=cnn_params['l_rate']), loss=cnn_params['loss'], metrics=['accuracy'])   \n",
    "    \n",
    "    return final_model\n",
    "\n",
    "def get_mlp_model(ip_dim):\n",
    "\n",
    "    mlp_model = Sequential()\n",
    "\n",
    "    mlp_model.add(Dense(units=cnn_params['op_unit'], activation=cnn_params['op_activ'], name='classif_op',\n",
    "                            input_dim=ip_dim))\n",
    "    mlp_model.compile(optimizer=Adam(learning_rate=cnn_params['l_rate']), loss=cnn_params['loss'],\n",
    "                          metrics=['accuracy'])\n",
    "    return mlp_model\n",
    "\n",
    "def get_prf1_score(y_true, y_pred):\n",
    "    tp, fp, fn = 0.0, 0.0, 0.0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            tp += 1\n",
    "        elif y_pred[i] > y_true[i]:\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    if tp == 0:\n",
    "        tp = 1.0\n",
    "    if fp == 0:\n",
    "        fp = 1.0\n",
    "    if fn == 0:\n",
    "        fn  = 1.0\n",
    "    P = tp / (tp + fp)\n",
    "    R = tp / (tp + fn)\n",
    "    F = 2 * P * R / (P + R)\n",
    "    return P, R, F\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:13:39.298032800Z",
     "start_time": "2024-03-25T03:13:39.280807300Z"
    }
   },
   "id": "abac5a9dc1eb9893",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Primary Dataset\n",
    "The study's primary dataset is 500 rows of de-identified social media posts discussing various aspects of self-destructive behavior. Each row has a user id, the text of the post, and a label describing how the post was manually classified.      "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bd279f28ea151cd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     User                                               Post       Label\n0  user-0  ['Its not a viable option, and youll be leavin...  Supportive\n1  user-1  ['It can be hard to appreciate the notion that...    Ideation",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User</th>\n      <th>Post</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>user-0</td>\n      <td>['Its not a viable option, and youll be leavin...</td>\n      <td>Supportive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>user-1</td>\n      <td>['It can be hard to appreciate the notion that...</td>\n      <td>Ideation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv ( ip_txt_file ).loc[:limit_rows]\n",
    "df.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:13:39.349563400Z",
     "start_time": "2024-03-25T03:13:39.286844600Z"
    }
   },
   "id": "a61a91470b2f4a5c",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleanup\n",
    "The original study provided code to do the word tokenization, but it was based on an older version of python and honestly wasn't very efficient. We've cleaned this up a bit by converting to pandas and using more pythonic transformations. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e50f24a6929889cd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         User                                               Post       Label\n0      user-0  ['Its not a viable option, and youll be leavin...  Supportive\n1      user-1  ['It can be hard to appreciate the notion that...    Ideation\n2      user-2  ['Hi, so last night i was sitting on the ledge...    Behavior\n3      user-3  ['I tried to kill my self once and failed badl...     Attempt\n4      user-4  ['Hi NEM3030. What sorts of things do you enjo...    Ideation\n..        ...                                                ...         ...\n121  user-121  ['No more ideas?', 'I dont agree with live for...    Ideation\n122  user-122  ['It started about two years ago. I dont feel ...    Ideation\n123  user-123  ['Theres a test for depression? I just went to...    Behavior\n124  user-124  ['Same, pm me and we can talk.', 'Hi. Im in th...     Attempt\n125  user-125  ['While people are bit burdened by my admittin...    Ideation\n\n[126 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User</th>\n      <th>Post</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>user-0</td>\n      <td>['Its not a viable option, and youll be leavin...</td>\n      <td>Supportive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>user-1</td>\n      <td>['It can be hard to appreciate the notion that...</td>\n      <td>Ideation</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>user-2</td>\n      <td>['Hi, so last night i was sitting on the ledge...</td>\n      <td>Behavior</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>user-3</td>\n      <td>['I tried to kill my self once and failed badl...</td>\n      <td>Attempt</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>user-4</td>\n      <td>['Hi NEM3030. What sorts of things do you enjo...</td>\n      <td>Ideation</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>user-121</td>\n      <td>['No more ideas?', 'I dont agree with live for...</td>\n      <td>Ideation</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>user-122</td>\n      <td>['It started about two years ago. I dont feel ...</td>\n      <td>Ideation</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>user-123</td>\n      <td>['Theres a test for depression? I just went to...</td>\n      <td>Behavior</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>user-124</td>\n      <td>['Same, pm me and we can talk.', 'Hi. Im in th...</td>\n      <td>Attempt</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>user-125</td>\n      <td>['While people are bit burdened by my admittin...</td>\n      <td>Ideation</td>\n    </tr>\n  </tbody>\n</table>\n<p>126 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:13:39.391094700Z",
     "start_time": "2024-03-25T03:13:39.338105700Z"
    }
   },
   "id": "9c29b9d37d50bce8",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [0 2 3 4 2]\n",
      "posts:  [['its' 'not' 'a' ... '@PADD' '@PADD' '@PADD']\n",
      " ['it' 'can' 'be' ... 'my' 'studies' 'and']\n",
      " ['hi' 'so' 'last' ... '@PADD' '@PADD' '@PADD']\n",
      " ['i' 'tried' 'to' ... '@PADD' '@PADD' '@PADD']\n",
      " ['hi' 'nem3030' 'what' ... '@PADD' '@PADD' '@PADD']]\n"
     ]
    }
   ],
   "source": [
    "df['post_clean'] = df.Post.str.lower() # convert to lowercase\n",
    "df['post_clean'] = df.post_clean.str.replace(\"/[^ -~]+/g\",\"\", regex=True) ## remove non-printable\n",
    "df[\"post_clean\"] = df.post_clean.str.replace('[^\\w\\s]','', regex=True) ## remove punctuation\n",
    "df[\"post_clean\"] = df.post_clean.apply(word_tokenize) ## tokenize\n",
    "df[\"post_clean\"] = df.post_clean[:sys_params['max_sent_len']] # limit length\n",
    "df[\"post_clean\"] = df.post_clean.apply ( lambda x : list(pad(x, sys_params['max_sent_len'], sys_params['str_padd']))) ## pad list\n",
    "\n",
    "df['LabelCode'] = df.Label.map ( severity_classes ) ## map labels to codes\n",
    "\n",
    "labels = np.array (df.LabelCode.values)\n",
    "print ( 'labels:', labels[:5] )\n",
    "\n",
    "posts = np.array(df.post_clean.values.tolist())\n",
    "print ( 'posts: ', posts[:5] )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:13:39.810005600Z",
     "start_time": "2024-03-25T03:13:39.353566700Z"
    }
   },
   "id": "20de4fe89ce51b49",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectorize\n",
    "Features are built by mapping word tokens to vectors of similar words. These similarities are taken from pre-built\n",
    "similarity vectors. For this study, the authors used a popular open-source project called ConceptNet. From their documentation:\n",
    "\n",
    "> ConceptNet is a freely-available semantic network, designed to help computers \n",
    "> understand the meanings of words that people use.\n",
    "\n",
    "We've improved on the study's code by allowing these vectors to be automatically downloaded and by caching word \n",
    "vectors to significantly reduce loading time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "261e0bc56e6517f6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached vectors.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<gensim.models.keyedvectors.KeyedVectors at 0x1817e628690>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (w2v_file['cache'] != \"\") & (os.path.isfile ( w2v_file['cache']) ):\n",
    "    print ( \"Using cached vectors.\" )\n",
    "    w2v_model = KeyedVectors.load(w2v_file['cache'])\n",
    "else:\n",
    "    if not os.path.isfile ( w2v_file['file'] ):\n",
    "        print ( f\"Could not find {w2v_file['file']}.. attempting download from {w2v_file['source']}.\" )\n",
    "        r = requests.get(w2v_file['source'], allow_redirects=True)\n",
    "        open ( w2v_file['file'], 'wb').write ( r.content )\n",
    "    \n",
    "    print ( \"Loading vectors... this will take a few minutes..\" )\n",
    "    w2v_model = KeyedVectors.load_word2vec_format(w2v_file['file'], binary=w2v_file['is_binary'], limit=w2v_file['limit'])\n",
    "    if w2v_file['cache'] != \"\":\n",
    "        w2v_model.save( w2v_file['cache'] )\n",
    "\n",
    "w2v_model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:13:40.249553Z",
     "start_time": "2024-03-25T03:13:39.807007500Z"
    }
   },
   "id": "b90756549c23528c",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 1500, 300) (126,)\n"
     ]
    }
   ],
   "source": [
    "vocab = w2v_model.key_to_index\n",
    "padding = np.zeros(sys_params['emb_dim'], dtype='float32')\n",
    "\n",
    "vectors = []\n",
    "for sentence in posts:\n",
    "    vector = []\n",
    "    for tok in sentence:\n",
    "        if tok==sys_params['str_padd']:\n",
    "            vector.append(list(padding))\n",
    "        \n",
    "        elif tok in vocab:\n",
    "            vector.append(w2v_model[tok].astype(float).tolist())\n",
    "    \n",
    "        else:\n",
    "            vector.append(list(padding))\n",
    "    \n",
    "    vectors.append(vector)   \n",
    "    \n",
    "x_data, y_data = np.array(vectors), np.array ( labels )\n",
    "print ( x_data.shape, y_data.shape )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:13:50.608809600Z",
     "start_time": "2024-03-25T03:13:40.249553Z"
    }
   },
   "id": "aad0986ce9f048b",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2e695c47d8855be"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x_data_all = x_data.reshape(x_data.shape[0], x_data.shape[1], x_data.shape[2], 1)\n",
    "y_data_all = labels\n",
    "\n",
    "np.savez_compressed ( f'./data/smalldataset.npz', x=x_data_all, y=y_data_all )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:14:03.009719700Z",
     "start_time": "2024-03-25T03:13:50.603824Z"
    }
   },
   "id": "f32eff9e11d2b0f2",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ed97736eb8dd5db"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 1500, 300, 1) (126,)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load ( './data/smalldataset.npz' )\n",
    "x_data_all = dataset['x']\n",
    "y_data_all = dataset['y']\n",
    "print ( x_data_all.shape, y_data_all.shape )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:14:04.408009800Z",
     "start_time": "2024-03-25T03:14:03.013722100Z"
    }
   },
   "id": "ac038a76bffc9759",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the Model (simple)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80819be329a0b036"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 1500, 300, 1) (84, 5)\n",
      "(42, 1500, 300, 1) (42, 5)\n",
      "Epoch 1/2\n",
      "21/21 [==============================] - 4s 130ms/step - loss: 1.5437 - accuracy: 0.3810\n",
      "Epoch 2/2\n",
      "21/21 [==============================] - 3s 136ms/step - loss: 1.3478 - accuracy: 0.4048\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000018180A63600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 1s 249ms/step\n",
      "2/2 [==============================] - 0s 93ms/step\n",
      "Epoch 1/2\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.5588 - accuracy: 0.2738\n",
      "Epoch 2/2\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.4665 - accuracy: 0.3929\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "\n",
      "Precision: 0.519\t Recall: 0.483\t F1-Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "y_data_categorized = to_categorical(y_data_all, num_classes=5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( x_data_all , y_data_categorized, test_size=0.33, random_state=42)\n",
    "print ( X_train.shape, y_train.shape )\n",
    "print ( X_test.shape, y_test.shape )\n",
    "\n",
    "cnn_model = get_keras_cnn_model()\n",
    "\n",
    "epochs = 2 #cnn_params['epoch']\n",
    "cnn_model.fit(x=X_train, y=y_train, batch_size=cnn_params['batch'], epochs=epochs, verbose=cnn_params['verbose'])\n",
    "\n",
    "model_feat_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.get_layer(intermediate_layer).output)\n",
    "train_cnn_feat = np.array(model_feat_extractor.predict(X_train))\n",
    "test_cnn_feat = np.array(model_feat_extractor.predict(X_test))\n",
    "\n",
    "mlp_model = get_mlp_model(ip_dim = len(train_cnn_feat[0]))\n",
    "mlp_model.fit(x=train_cnn_feat, y=y_train, batch_size=cnn_params['batch'], epochs=epochs, verbose=cnn_params['verbose'])\n",
    "\n",
    "y_pred = mlp_model.predict(test_cnn_feat)\n",
    "y_pred_am = np.argmax(y_pred, axis=-1)\n",
    "y_test_am = np.argmax(y_test, axis=-1)\n",
    "\n",
    "precision, recall, f1 = get_prf1_score(y_test_am, y_pred_am)\n",
    "print ('\\nPrecision: {0}\\t Recall: {1}\\t F1-Score: {2}'\\\n",
    "    .format(round(precision,3), round(recall,3), round(f1,3)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:16:03.636737Z",
     "start_time": "2024-03-25T03:15:54.367565300Z"
    }
   },
   "id": "750b0c73c95973f2",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the Model (KFold Cross Validation)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "818d09ab5ea74953"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Stratified Cross Validation: 1/5...\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 5s 127ms/step - loss: 1.6411 - accuracy: 0.3500\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 1.4352 - accuracy: 0.4700\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 3s 139ms/step - loss: 1.2895 - accuracy: 0.4300\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 1.1335 - accuracy: 0.5700\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 0.9621 - accuracy: 0.7500\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000018180CB02C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "4/4 [==============================] - 1s 202ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.5570 - accuracy: 0.3400\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.4291 - accuracy: 0.3700\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.3582 - accuracy: 0.3700\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.2917 - accuracy: 0.4100\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.2379 - accuracy: 0.4300\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "\n",
      "Precision: 0.556\t Recall: 0.556\t F1-Score: 0.556\n",
      "\n",
      "Running Stratified Cross Validation: 2/5...\n",
      "Epoch 1/5\n",
      "26/26 [==============================] - 4s 129ms/step - loss: 0.9642 - accuracy: 0.7921\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 3s 126ms/step - loss: 0.8373 - accuracy: 0.7723\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 3s 129ms/step - loss: 0.6191 - accuracy: 0.9010\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 3s 129ms/step - loss: 0.4633 - accuracy: 0.9208\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 3s 128ms/step - loss: 0.3221 - accuracy: 1.0000\n",
      "4/4 [==============================] - 1s 202ms/step\n",
      "1/1 [==============================] - 0s 237ms/step\n",
      "Epoch 1/5\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.5242 - accuracy: 0.3366\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.3620 - accuracy: 0.3861\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.2173 - accuracy: 0.5347\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.1107 - accuracy: 0.6931\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.9882 - accuracy: 0.7228\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "\n",
      "Precision: 0.765\t Recall: 0.619\t F1-Score: 0.684\n",
      "\n",
      "Running Stratified Cross Validation: 3/5...\n",
      "Epoch 1/5\n",
      "26/26 [==============================] - 3s 131ms/step - loss: 0.3596 - accuracy: 0.9703\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 3s 127ms/step - loss: 0.2835 - accuracy: 0.9901\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 4s 151ms/step - loss: 0.1928 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 5s 185ms/step - loss: 0.1287 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 4s 144ms/step - loss: 0.1082 - accuracy: 1.0000\n",
      "4/4 [==============================] - 1s 205ms/step\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "Epoch 1/5\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.5986 - accuracy: 0.2772 \n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.3572 - accuracy: 0.3861\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.1694 - accuracy: 0.5248\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.0149 - accuracy: 0.8713\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.8761 - accuracy: 0.8515\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "\n",
      "Precision: 0.957\t Recall: 0.917\t F1-Score: 0.936\n",
      "\n",
      "Running Stratified Cross Validation: 4/5...\n",
      "Epoch 1/5\n",
      "26/26 [==============================] - 4s 134ms/step - loss: 0.1173 - accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 4s 141ms/step - loss: 0.0869 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 3s 130ms/step - loss: 0.0712 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 3s 131ms/step - loss: 0.0529 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 3s 133ms/step - loss: 0.0421 - accuracy: 1.0000\n",
      "4/4 [==============================] - 1s 312ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "Epoch 1/5\n",
      "26/26 [==============================] - 1s 2ms/step - loss: 1.5166 - accuracy: 0.3168\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.2852 - accuracy: 0.3960\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.1026 - accuracy: 0.4653\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.9415 - accuracy: 0.8119\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.7935 - accuracy: 0.9208\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "\n",
      "Precision: 0.96\t Recall: 0.96\t F1-Score: 0.96\n",
      "\n",
      "Running Stratified Cross Validation: 5/5...\n",
      "Epoch 1/5\n",
      "26/26 [==============================] - 4s 133ms/step - loss: 0.0503 - accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 3s 126ms/step - loss: 0.0527 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 3s 126ms/step - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 3s 133ms/step - loss: 0.0380 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 3s 132ms/step - loss: 0.0300 - accuracy: 1.0000\n",
      "4/4 [==============================] - 1s 208ms/step\n",
      "1/1 [==============================] - 0s 311ms/step\n",
      "Epoch 1/5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.6706 - accuracy: 0.1980 \n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.3040 - accuracy: 0.4851\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.1148 - accuracy: 0.6139\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.9610 - accuracy: 0.8614\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.8011 - accuracy: 0.9208\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "\n",
      "Precision: 0.962\t Recall: 0.962\t F1-Score: 0.962\n"
     ]
    },
    {
     "data": {
      "text/plain": "         Precision    Recall        F1\nI                                     \n1         0.555556  0.555556  0.555556\n2         0.764706  0.619048  0.684211\n3         0.956522  0.916667  0.936170\n4         0.960000  0.960000  0.960000\n5         0.961538  0.961538  0.961538\nAverage   0.839664  0.802562  0.819495",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n    <tr>\n      <th>I</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.555556</td>\n      <td>0.555556</td>\n      <td>0.555556</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.764706</td>\n      <td>0.619048</td>\n      <td>0.684211</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.956522</td>\n      <td>0.916667</td>\n      <td>0.936170</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.960000</td>\n      <td>0.960000</td>\n      <td>0.960000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.961538</td>\n      <td>0.961538</td>\n      <td>0.961538</td>\n    </tr>\n    <tr>\n      <th>Average</th>\n      <td>0.839664</td>\n      <td>0.802562</td>\n      <td>0.819495</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs=5\n",
    "scores = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=sys_params['cross_val'])\n",
    "skf.get_n_splits(x_data_all, y_data_all)\n",
    "\n",
    "cnn_model = get_keras_cnn_model()\n",
    "\n",
    "for cv_count, (train_index, test_index) in enumerate(skf.split(x_data_all, y_data_all), start=1):\n",
    "    print ('\\nRunning Stratified Cross Validation: {0}/{1}...'.format(cv_count, sys_params['cross_val']))\n",
    "\n",
    "    X_train, X_test = x_data_all[train_index], x_data_all[test_index]\n",
    "    y_train, y_test = y_data_all[train_index], y_data_all[test_index]\n",
    "    \n",
    "    y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "    cnn_model.fit(x=X_train, y=y_train, batch_size=cnn_params['batch'], epochs=epochs, verbose=cnn_params['verbose'])\n",
    "    model_feat_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.get_layer(intermediate_layer).output)\n",
    "    train_cnn_feat = model_feat_extractor.predict(X_train)\n",
    "    test_cnn_feat = model_feat_extractor.predict(X_test)\n",
    "\n",
    "    mlp_model = get_mlp_model(ip_dim = len(train_cnn_feat[0]))\n",
    "    mlp_model.fit(x=train_cnn_feat, y=y_train, batch_size=cnn_params['batch'], epochs=epochs, verbose=cnn_params['verbose'])\n",
    "    \n",
    "    y_pred = mlp_model.predict(test_cnn_feat)\n",
    "    y_pred_am = np.argmax(y_pred, axis=-1)\n",
    "    y_test_am = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    precision, recall, f1 = get_prf1_score(y_test_am, y_pred_am)\n",
    "    print ('\\nPrecision: {0}\\t Recall: {1}\\t F1-Score: {2}'\\\n",
    "        .format(round(precision,3), round(recall,3), round(f1,3)))\n",
    "\n",
    "    scores.append({'I':cv_count, 'Precision':precision, 'Recall':recall, 'F1':f1})\n",
    "\n",
    "sumdf = pd.DataFrame ( scores ).set_index('I')\n",
    "sumdf.loc['Average'] = sumdf.mean()\n",
    "display ( sumdf )\n",
    "\n",
    "# print ('\\nAverage Precision: {0}\\t Recall: {1}\\t F1-Score: {2}'\\\n",
    "#        .format(round(sumdf.P.mean(),3), round(sumdf.R.mean(),3), round(sumdf.F.mean(),3)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:18:33.585584300Z",
     "start_time": "2024-03-25T03:16:50.784714900Z"
    }
   },
   "id": "397dd2ef261feea9",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "|"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-25T03:14:04.814502300Z"
    }
   },
   "id": "c2f89db656eacd7b",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
