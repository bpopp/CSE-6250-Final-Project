{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Input, MaxPool2D\n",
    "from keras.layers import Conv2D, GlobalAveragePooling1D, MaxPooling2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "## our imports\n",
    "import pandas as pd\n",
    "import re\n",
    "from itertools import chain, repeat, islice\n",
    "import requests\n",
    "\n",
    "## utility functions\n",
    "def pad_infinite(iterable, padding=None):\n",
    "   return chain(iterable, repeat(padding))\n",
    "\n",
    "def pad(iterable, size, padding=None):\n",
    "   return islice(pad_infinite(iterable, padding), size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:29:42.596238400Z",
     "start_time": "2024-03-25T02:29:42.588032Z"
    }
   },
   "id": "ab31492270c2c5cd",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Using the same variable declarations and configuration options as the original study\n",
    "punctuations = list(string.punctuation)\n",
    "ip_txt_file = './data/500_Reddit_users_posts_labels.csv'  # CSV file: \"User\", \"Post\", \"Label\"\n",
    "ip_feat_file = './data/External_Features.csv'             # CSV file: \"User\", \"Features\"\n",
    "limit_rows = 125   ## used to build a partial dataset\n",
    "\n",
    "w2v_file = {'file': './out/numberbatch-en-19.08.txt.gz', \n",
    "            'is_binary': False, \n",
    "            'limit': None, \n",
    "            'cache': './out/vectors.kv', \n",
    "            'source':'https://conceptnet.s3.amazonaws.com/downloads/2019/numberbatch/numberbatch-en-19.08.txt.gz' }\n",
    "\n",
    "op_file = './out/Result_5-Label_Classification.tsv'\n",
    "severity_classes = {'Supportive': 0, 'Indicator': 1, 'Ideation': 2, 'Behavior': 3, 'Attempt': 4}\n",
    "\n",
    "sys_params = {'emb_dim': 300,\n",
    "              'max_sent_len': 1500,\n",
    "              'str_padd': '@PADD',\n",
    "              'cross_val': 5}\n",
    "\n",
    "cnn_params = {'no_filters': 100,\n",
    "              'kernels': [3, 4, 5],\n",
    "              'channel': 1,\n",
    "              'c_stride': (1, sys_params['emb_dim']),\n",
    "              'pad': 'same',\n",
    "              'ip_shape': (sys_params['max_sent_len'], sys_params['emb_dim'], 1),\n",
    "              'c_activ': 'relu',\n",
    "              'drop_rate': 0.3,\n",
    "              'dense_1_unit': 128,\n",
    "              'dense_2_unit': 128,\n",
    "              'dense_activ': 'relu',\n",
    "              'op_unit': 5,             # 5-Label classification\n",
    "              'op_activ': 'softmax',\n",
    "              'l_rate': 0.001,\n",
    "              'loss': 'categorical_crossentropy',\n",
    "              'batch': 4,\n",
    "              'epoch': 50,\n",
    "              'verbose': 1}\n",
    "\n",
    "intermediate_layer = 'flat_drop'    # for extracting features from CNN"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:29:49.965115400Z",
     "start_time": "2024-03-25T02:29:49.957223100Z"
    }
   },
   "id": "96ae94470d964fc4",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_keras_cnn_model():\n",
    "\n",
    "    l_ip = Input(shape=(sys_params['max_sent_len'], sys_params['emb_dim'], 1), dtype='float32')\n",
    "    lst_convfeat = []\n",
    "    for filter in cnn_params['kernels']:\n",
    "        l_conv = Conv2D(filters=cnn_params['no_filters'], kernel_size=(filter, sys_params['emb_dim']), strides=cnn_params['c_stride'],\n",
    "                        padding=cnn_params['pad'], data_format='channels_last', input_shape=cnn_params['ip_shape'],\n",
    "                        activation=cnn_params['c_activ'])(l_ip)\n",
    "        l_pool = MaxPool2D(pool_size=(sys_params['max_sent_len'], 1))(l_conv)\n",
    "        lst_convfeat.append(l_pool)\n",
    "        \n",
    "    l_concat = Concatenate(axis=1)(lst_convfeat)\n",
    "    l_flat = Flatten()(l_concat)\n",
    "    l_drop = Dropout(rate=cnn_params['drop_rate'], name='flat_drop')(l_flat)\n",
    "    l_op = Dense(units=cnn_params['op_unit'], activation=cnn_params['op_activ'], name='cnn_op')(l_drop)\n",
    "    \n",
    "    final_model = Model(l_ip, l_op)\n",
    "    final_model.compile(optimizer=Adam(learning_rate=cnn_params['l_rate']), loss=cnn_params['loss'], metrics=['accuracy'])   \n",
    "    \n",
    "    return final_model\n",
    "\n",
    "def get_mlp_model(ip_dim):\n",
    "\n",
    "    mlp_model = Sequential()\n",
    "\n",
    "    mlp_model.add(Dense(units=cnn_params['op_unit'], activation=cnn_params['op_activ'], name='classif_op',\n",
    "                            input_dim=ip_dim))\n",
    "    mlp_model.compile(optimizer=Adam(learning_rate=cnn_params['l_rate']), loss=cnn_params['loss'],\n",
    "                          metrics=['accuracy'])\n",
    "    return mlp_model\n",
    "\n",
    "def get_prf1_score(y_true, y_pred):\n",
    "    tp, fp, fn = 0.0, 0.0, 0.0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            tp += 1\n",
    "        elif y_pred[i] > y_true[i]:\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    if tp == 0:\n",
    "        tp = 1.0\n",
    "    if fp == 0:\n",
    "        fp = 1.0\n",
    "    if fn == 0:\n",
    "        fn  = 1.0\n",
    "    P = tp / (tp + fp)\n",
    "    R = tp / (tp + fn)\n",
    "    F = 2 * P * R / (P + R)\n",
    "    return P, R, F\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:29:50.451449800Z",
     "start_time": "2024-03-25T02:29:50.419093500Z"
    }
   },
   "id": "abac5a9dc1eb9893",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Primary Dataset\n",
    "The study's primary dataset is 500 rows of de-identified social media posts discussing various aspects of self-destructive behavior. Each row has a user id, the text of the post, and a label describing how the post was manually classified.      "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bd279f28ea151cd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     User                                               Post       Label\n0  user-0  ['Its not a viable option, and youll be leavin...  Supportive\n1  user-1  ['It can be hard to appreciate the notion that...    Ideation",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User</th>\n      <th>Post</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>user-0</td>\n      <td>['Its not a viable option, and youll be leavin...</td>\n      <td>Supportive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>user-1</td>\n      <td>['It can be hard to appreciate the notion that...</td>\n      <td>Ideation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv ( ip_txt_file ).loc[:limit_rows]\n",
    "df.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:29:52.015763600Z",
     "start_time": "2024-03-25T02:29:51.956452800Z"
    }
   },
   "id": "a61a91470b2f4a5c",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleanup\n",
    "The original study provided code to do the word tokenization, but it was based on an older version of python and honestly wasn't very efficient. We've cleaned this up a bit by converting to pandas and using more pythonic transformations. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e50f24a6929889cd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         User                                               Post       Label\n0      user-0  ['Its not a viable option, and youll be leavin...  Supportive\n1      user-1  ['It can be hard to appreciate the notion that...    Ideation\n2      user-2  ['Hi, so last night i was sitting on the ledge...    Behavior\n3      user-3  ['I tried to kill my self once and failed badl...     Attempt\n4      user-4  ['Hi NEM3030. What sorts of things do you enjo...    Ideation\n..        ...                                                ...         ...\n121  user-121  ['No more ideas?', 'I dont agree with live for...    Ideation\n122  user-122  ['It started about two years ago. I dont feel ...    Ideation\n123  user-123  ['Theres a test for depression? I just went to...    Behavior\n124  user-124  ['Same, pm me and we can talk.', 'Hi. Im in th...     Attempt\n125  user-125  ['While people are bit burdened by my admittin...    Ideation\n\n[126 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User</th>\n      <th>Post</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>user-0</td>\n      <td>['Its not a viable option, and youll be leavin...</td>\n      <td>Supportive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>user-1</td>\n      <td>['It can be hard to appreciate the notion that...</td>\n      <td>Ideation</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>user-2</td>\n      <td>['Hi, so last night i was sitting on the ledge...</td>\n      <td>Behavior</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>user-3</td>\n      <td>['I tried to kill my self once and failed badl...</td>\n      <td>Attempt</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>user-4</td>\n      <td>['Hi NEM3030. What sorts of things do you enjo...</td>\n      <td>Ideation</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>user-121</td>\n      <td>['No more ideas?', 'I dont agree with live for...</td>\n      <td>Ideation</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>user-122</td>\n      <td>['It started about two years ago. I dont feel ...</td>\n      <td>Ideation</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>user-123</td>\n      <td>['Theres a test for depression? I just went to...</td>\n      <td>Behavior</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>user-124</td>\n      <td>['Same, pm me and we can talk.', 'Hi. Im in th...</td>\n      <td>Attempt</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>user-125</td>\n      <td>['While people are bit burdened by my admittin...</td>\n      <td>Ideation</td>\n    </tr>\n  </tbody>\n</table>\n<p>126 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:29:53.273328700Z",
     "start_time": "2024-03-25T02:29:53.255134400Z"
    }
   },
   "id": "9c29b9d37d50bce8",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [0 2 3 4 2]\n",
      "posts:  [['its' 'not' 'a' ... '@PADD' '@PADD' '@PADD']\n",
      " ['it' 'can' 'be' ... 'my' 'studies' 'and']\n",
      " ['hi' 'so' 'last' ... '@PADD' '@PADD' '@PADD']\n",
      " ['i' 'tried' 'to' ... '@PADD' '@PADD' '@PADD']\n",
      " ['hi' 'nem3030' 'what' ... '@PADD' '@PADD' '@PADD']]\n"
     ]
    }
   ],
   "source": [
    "df['post_clean'] = df.Post.str.lower() # convert to lowercase\n",
    "df['post_clean'] = df.post_clean.str.replace(\"/[^ -~]+/g\",\"\", regex=True) ## remove non-printable\n",
    "df[\"post_clean\"] = df.post_clean.str.replace('[^\\w\\s]','', regex=True) ## remove punctuation\n",
    "df[\"post_clean\"] = df.post_clean.apply(word_tokenize) ## tokenize\n",
    "df[\"post_clean\"] = df.post_clean[:sys_params['max_sent_len']] # limit length\n",
    "df[\"post_clean\"] = df.post_clean.apply ( lambda x : list(pad(x, sys_params['max_sent_len'], sys_params['str_padd']))) ## pad list\n",
    "\n",
    "df['LabelCode'] = df.Label.map ( severity_classes ) ## map labels to codes\n",
    "\n",
    "labels = np.array (df.LabelCode.values)\n",
    "print ( 'labels:', labels[:5] )\n",
    "\n",
    "posts = np.array(df.post_clean.values.tolist())\n",
    "print ( 'posts: ', posts[:5] )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:29:54.777500400Z",
     "start_time": "2024-03-25T02:29:54.046547600Z"
    }
   },
   "id": "20de4fe89ce51b49",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectorize\n",
    "Features are built by mapping word tokens to vectors of similar words. These similarities are taken from pre-built\n",
    "similarity vectors. For this study, the authors used a popular open-source project called ConceptNet. From their documentation:\n",
    "\n",
    "> ConceptNet is a freely-available semantic network, designed to help computers \n",
    "> understand the meanings of words that people use.\n",
    "\n",
    "We've improved on the study's code by allowing these vectors to be automatically downloaded and by caching word \n",
    "vectors to significantly reduce loading time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "261e0bc56e6517f6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached vectors.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<gensim.models.keyedvectors.KeyedVectors at 0x244624e4d10>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (w2v_file['cache'] != \"\") & (os.path.isfile ( w2v_file['cache']) ):\n",
    "    print ( \"Using cached vectors.\" )\n",
    "    w2v_model = KeyedVectors.load(w2v_file['cache'])\n",
    "else:\n",
    "    if not os.path.isfile ( w2v_file['file'] ):\n",
    "        print ( f\"Could not find {w2v_file['file']}.. attempting download from {w2v_file['source']}.\" )\n",
    "        r = requests.get(w2v_file['source'], allow_redirects=True)\n",
    "        open ( w2v_file['file'], 'wb').write ( r.content )\n",
    "    \n",
    "    print ( \"Loading vectors... this will take a few minutes..\" )\n",
    "    w2v_model = KeyedVectors.load_word2vec_format(w2v_file['file'], binary=w2v_file['is_binary'], limit=w2v_file['limit'])\n",
    "    if w2v_file['cache'] != \"\":\n",
    "        w2v_model.save( w2v_file['cache'] )\n",
    "\n",
    "w2v_model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:29:55.890954300Z",
     "start_time": "2024-03-25T02:29:55.504349Z"
    }
   },
   "id": "b90756549c23528c",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 1500, 300) (126,)\n"
     ]
    }
   ],
   "source": [
    "vocab = w2v_model.key_to_index\n",
    "padding = np.zeros(sys_params['emb_dim'], dtype='float32')\n",
    "\n",
    "vectors = []\n",
    "for sentence in posts:\n",
    "    vector = []\n",
    "    for tok in sentence:\n",
    "        if tok==sys_params['str_padd']:\n",
    "            vector.append(list(padding))\n",
    "        \n",
    "        elif tok in vocab:\n",
    "            vector.append(w2v_model[tok].astype(float).tolist())\n",
    "    \n",
    "        else:\n",
    "            vector.append(list(padding))\n",
    "    \n",
    "    vectors.append(vector)   \n",
    "    \n",
    "x_data, y_data = np.array(vectors), np.array ( labels )\n",
    "print ( x_data.shape, y_data.shape )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:30:06.669122900Z",
     "start_time": "2024-03-25T02:29:56.446207700Z"
    }
   },
   "id": "aad0986ce9f048b",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2e695c47d8855be"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 1500, 300, 1) (84, 5)\n",
      "(42, 1500, 300, 1) (42, 5)\n"
     ]
    }
   ],
   "source": [
    "x_data_all = x_data.reshape(x_data.shape[0], x_data.shape[1], x_data.shape[2], 1)\n",
    "y_data_all = to_categorical(labels, num_classes=5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( x_data_all, y_data_all, test_size=0.33, random_state=42)\n",
    "print ( X_train.shape, y_train.shape )\n",
    "print ( X_test.shape, y_test.shape )\n",
    "\n",
    "np.savez_compressed ( f'./data/dataset.npz', X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test )\n",
    "\n",
    "\n",
    "# print (cnn_model.summary())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:30:19.073868600Z",
     "start_time": "2024-03-25T02:30:06.658595800Z"
    }
   },
   "id": "f32eff9e11d2b0f2",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ed97736eb8dd5db"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335, 1500, 300, 1) (335, 5)\n",
      "(165, 1500, 300, 1) (165, 5)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load ( './data/dataset.npz' )\n",
    "X_train, y_train, X_test, y_test = dataset['X_train'],dataset['y_train'],dataset['X_test'],dataset['y_test']\n",
    "print ( X_train.shape, y_train.shape )\n",
    "print ( X_test.shape, y_test.shape )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T01:04:08.362792400Z",
     "start_time": "2024-03-25T01:04:01.984352200Z"
    }
   },
   "id": "ac038a76bffc9759",
   "execution_count": 298
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the Model (simple)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80819be329a0b036"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "100/100 [==============================] - 24s 228ms/step - loss: 1.5630 - accuracy: 0.3375\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 1.4144 - accuracy: 0.4325\n",
      "13/13 [==============================] - 6s 417ms/step\n",
      "4/4 [==============================] - 2s 328ms/step\n",
      "Epoch 1/2\n",
      "100/100 [==============================] - 1s 4ms/step - loss: 1.5318 - accuracy: 0.3375\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 1.4488 - accuracy: 0.4100\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "\n",
      "Precision: 0.5\t Recall: 0.587\t F1-Score: 0.54\n"
     ]
    }
   ],
   "source": [
    "cnn_model = get_keras_cnn_model()\n",
    "\n",
    "epochs = 2 #cnn_params['epoch']\n",
    "cnn_model.fit(x=X_train, y=y_train, batch_size=cnn_params['batch'], epochs=epochs, verbose=cnn_params['verbose'])\n",
    "\n",
    "model_feat_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.get_layer(intermediate_layer).output)\n",
    "train_cnn_feat = np.array(model_feat_extractor.predict(X_train))\n",
    "test_cnn_feat = np.array(model_feat_extractor.predict(X_test))\n",
    "\n",
    "mlp_model = get_mlp_model(ip_dim = len(train_cnn_feat[0]))\n",
    "mlp_model.fit(x=train_cnn_feat, y=y_train, batch_size=cnn_params['batch'], epochs=epochs, verbose=cnn_params['verbose'])\n",
    "\n",
    "y_pred = mlp_model.predict(test_cnn_feat)\n",
    "y_pred_am = np.argmax(y_pred, axis=-1)\n",
    "y_test_am = np.argmax(y_test, axis=-1)\n",
    "\n",
    "precision, recall, f1 = get_prf1_score(y_test_am, y_pred_am)\n",
    "print ('\\nPrecision: {0}\\t Recall: {1}\\t F1-Score: {2}'\\\n",
    "    .format(round(precision,3), round(recall,3), round(f1,3)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:04:32.108213200Z",
     "start_time": "2024-03-25T02:03:31.645348500Z"
    }
   },
   "id": "750b0c73c95973f2",
   "execution_count": 334
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the Model (KFold Cross Validation)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "818d09ab5ea74953"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Stratified Cross Validation: 1/5...\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 1.5584 - accuracy: 0.3275\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 13s 132ms/step - loss: 1.4066 - accuracy: 0.3975\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 1.2373 - accuracy: 0.5075\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 1.0225 - accuracy: 0.6550\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.8045 - accuracy: 0.7825\n",
      "13/13 [==============================] - 4s 282ms/step\n",
      "4/4 [==============================] - 1s 204ms/step\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.4710 - accuracy: 0.4125\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.2090 - accuracy: 0.5975\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.0103 - accuracy: 0.7475\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.8573 - accuracy: 0.8325\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.7362 - accuracy: 0.8850\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "\n",
      "Precision: 0.652\t Recall: 0.558\t F1-Score: 0.601\n",
      "\n",
      "Running Stratified Cross Validation: 2/5...\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 13s 133ms/step - loss: 0.8209 - accuracy: 0.7475\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 13s 132ms/step - loss: 0.6667 - accuracy: 0.8200\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.4581 - accuracy: 0.9175\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 14s 145ms/step - loss: 0.3302 - accuracy: 0.9525\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.2220 - accuracy: 0.9900\n",
      "13/13 [==============================] - 4s 277ms/step\n",
      "4/4 [==============================] - 1s 203ms/step\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 1s 1ms/step - loss: 1.3342 - accuracy: 0.4925\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.9581 - accuracy: 0.8275\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6878 - accuracy: 0.9375\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.5184 - accuracy: 0.9800\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3948 - accuracy: 1.0000\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "\n",
      "Precision: 0.966\t Recall: 0.876\t F1-Score: 0.919\n",
      "\n",
      "Running Stratified Cross Validation: 3/5...\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 13s 132ms/step - loss: 0.2844 - accuracy: 0.9450\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 14s 144ms/step - loss: 0.1886 - accuracy: 0.9875\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.1174 - accuracy: 0.9950\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.1028 - accuracy: 0.9925\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 0.0925 - accuracy: 0.9975\n",
      "13/13 [==============================] - 4s 291ms/step\n",
      "4/4 [==============================] - 1s 222ms/step\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.3110 - accuracy: 0.5550\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.8632 - accuracy: 0.8350\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.5973 - accuracy: 0.9650\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.4199 - accuracy: 0.9900\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3071 - accuracy: 1.0000\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "\n",
      "Precision: 0.99\t Recall: 0.99\t F1-Score: 0.99\n",
      "\n",
      "Running Stratified Cross Validation: 4/5...\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 15s 149ms/step - loss: 0.0929 - accuracy: 0.9950\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.0639 - accuracy: 0.9975\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 14s 143ms/step - loss: 0.0520 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 13s 133ms/step - loss: 0.0473 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.0398 - accuracy: 1.0000\n",
      "13/13 [==============================] - 4s 288ms/step\n",
      "4/4 [==============================] - 1s 212ms/step\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.3521 - accuracy: 0.5525\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.8463 - accuracy: 0.8625\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.5432 - accuracy: 0.9750\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3700 - accuracy: 0.9900\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2626 - accuracy: 1.0000\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "\n",
      "Precision: 0.99\t Recall: 0.99\t F1-Score: 0.99\n",
      "\n",
      "Running Stratified Cross Validation: 5/5...\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.0425 - accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.0313 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.0275 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "13/13 [==============================] - 4s 282ms/step\n",
      "4/4 [==============================] - 1s 202ms/step\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 1s 1ms/step - loss: 1.3554 - accuracy: 0.5100\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.8169 - accuracy: 0.8625\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.5023 - accuracy: 0.9925\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3318 - accuracy: 0.9975\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2299 - accuracy: 1.0000\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "\n",
      "Precision: 0.99\t Recall: 0.99\t F1-Score: 0.99\n"
     ]
    },
    {
     "data": {
      "text/plain": "         Precision    Recall        F1\nI                                     \n1         0.651515  0.558442  0.601399\n2         0.965909  0.876289  0.918919\n3         0.990099  0.990099  0.990099\n4         0.990099  0.990099  0.990099\n5         0.990099  0.990099  0.990099\nAverage   0.917544  0.881005  0.898123",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n    <tr>\n      <th>I</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.651515</td>\n      <td>0.558442</td>\n      <td>0.601399</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.965909</td>\n      <td>0.876289</td>\n      <td>0.918919</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.990099</td>\n      <td>0.990099</td>\n      <td>0.990099</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.990099</td>\n      <td>0.990099</td>\n      <td>0.990099</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.990099</td>\n      <td>0.990099</td>\n      <td>0.990099</td>\n    </tr>\n    <tr>\n      <th>Average</th>\n      <td>0.917544</td>\n      <td>0.881005</td>\n      <td>0.898123</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs=5\n",
    "scores = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=sys_params['cross_val'])\n",
    "skf.get_n_splits(x_data, y_data)\n",
    "\n",
    "cnn_model = get_keras_cnn_model()\n",
    "\n",
    "for cv_count, (train_index, test_index) in enumerate(skf.split(x_data, y_data), start=1):\n",
    "    print ('\\nRunning Stratified Cross Validation: {0}/{1}...'.format(cv_count, sys_params['cross_val']))\n",
    "\n",
    "    X_train, X_test = x_data_all[train_index], x_data_all[test_index]\n",
    "    y_train, y_test = y_data_all[train_index], y_data_all[test_index]\n",
    "    \n",
    "    cnn_model.fit(x=X_train, y=y_train, batch_size=cnn_params['batch'], epochs=epochs, verbose=cnn_params['verbose'])\n",
    "    model_feat_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.get_layer(intermediate_layer).output)\n",
    "    train_cnn_feat = model_feat_extractor.predict(X_train)\n",
    "    test_cnn_feat = model_feat_extractor.predict(X_test)\n",
    "\n",
    "    mlp_model = get_mlp_model(ip_dim = len(train_cnn_feat[0]))\n",
    "    mlp_model.fit(x=train_cnn_feat, y=y_train, batch_size=cnn_params['batch'], epochs=epochs, verbose=cnn_params['verbose'])\n",
    "    \n",
    "    y_pred = mlp_model.predict(test_cnn_feat)\n",
    "    y_pred_am = np.argmax(y_pred, axis=-1)\n",
    "    y_test_am = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    precision, recall, f1 = get_prf1_score(y_test_am, y_pred_am)\n",
    "    print ('\\nPrecision: {0}\\t Recall: {1}\\t F1-Score: {2}'\\\n",
    "        .format(round(precision,3), round(recall,3), round(f1,3)))\n",
    "\n",
    "    scores.append({'I':cv_count, 'Precision':precision, 'Recall':recall, 'F1':f1})\n",
    "\n",
    "sumdf = pd.DataFrame ( scores ).set_index('I')\n",
    "sumdf.loc['Average'] = sumdf.mean()\n",
    "display ( sumdf )\n",
    "\n",
    "# print ('\\nAverage Precision: {0}\\t Recall: {1}\\t F1-Score: {2}'\\\n",
    "#        .format(round(sumdf.P.mean(),3), round(sumdf.R.mean(),3), round(sumdf.F.mean(),3)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:00:38.910157Z",
     "start_time": "2024-03-25T01:54:16.217782500Z"
    }
   },
   "id": "397dd2ef261feea9",
   "execution_count": 332
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "|"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2f89db656eacd7b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
